{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import gc\n",
    "import time\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import KFold, GroupKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_log_mae(y_true, y_pred, floor=1e-9):\n",
    "    mae = (y_true-y_pred).abs().mean()\n",
    "    return np.log(max(mae, floor))\n",
    "\n",
    "\n",
    "def train_model(X, y, X_test, params, is_plot):\n",
    "    result_dict = {}\n",
    "    oof = np.zeros(len(X))\n",
    "    prediction = np.zeros(len(X_test))\n",
    "    scores = []\n",
    "    mae_scores = []\n",
    "    feature_importance = pd.DataFrame()\n",
    "\n",
    "    folds = GroupKFold(n_splits=5)\n",
    "\n",
    "    grouping = LabelEncoder()\n",
    "    grouping.fit(list(X['molecule_name'].values))\n",
    "    X['molecule_name'] = grouping.transform(list(X['molecule_name'].values))\n",
    "    groups = X['molecule_name'].values\n",
    "    X = X.drop('molecule_name', axis=1)\n",
    "\n",
    "    features = X.columns\n",
    "\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(folds.split(X, y, groups)):\n",
    "        print(f'\\nFold {fold + 1} started at {time.ctime()}')\n",
    "\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "        model = lgb.LGBMRegressor(**params)\n",
    "\n",
    "        model.fit(X_train, y_train,\n",
    "                  eval_set=[(X_train, y_train), (X_val, y_val)],\n",
    "                  eval_metric='mae',\n",
    "                  early_stopping_rounds=200,\n",
    "                  verbose=2000)\n",
    "\n",
    "        y_pred_val = model.predict(X_val)\n",
    "        y_pred = model.predict(X_test)\n",
    "        oof[val_idx] = y_pred_val.reshape(-1)\n",
    "\n",
    "        scores.append(mean_log_mae(y_val, y_pred_val))\n",
    "        mae_scores.append(model.best_score_['valid_1']['l1'])\n",
    "\n",
    "        prediction += y_pred\n",
    "\n",
    "        fold_importance = pd.DataFrame()\n",
    "        fold_importance['feature'] = features\n",
    "        fold_importance['importance'] = model.feature_importances_\n",
    "        fold_importance['fold'] = fold + 1\n",
    "        feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n",
    "\n",
    "    prediction /= folds.n_splits\n",
    "\n",
    "    print('\\nCV mean mae: {0: .4f}, std: {1: .4f}.'.format(np.mean(mae_scores), np.std(mae_scores)))\n",
    "    print('\\nCV mean score: {0: .4f}, std: {1: .4f}.'.format(np.mean(scores), np.std(scores)))\n",
    "\n",
    "    cols = feature_importance[['feature', 'importance']].groupby('feature').mean().sort_values(by='importance', ascending=False).index\n",
    "    best_features = feature_importance[['feature', 'importance']].groupby('feature').mean().sort_values(by='importance', ascending=False).reset_index()\n",
    "\n",
    "    result_dict['oof'] = oof\n",
    "    result_dict['prediction'] = prediction\n",
    "    result_dict['scores'] = scores\n",
    "    result_dict['mae'] = mae_scores\n",
    "    result_dict['feature_importance'] = best_features\n",
    "    \n",
    "    if is_plot:\n",
    "        plt.figure(figsize=(16,128))\n",
    "        sns.barplot(x='importance', y='feature', data=best_features)\n",
    "        plt.title('LGB Features (avg over folds)')\n",
    "        \n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    '1JHC': {\n",
    "        'n_estimators': 100000,\n",
    "        'metric': 'mae',\n",
    "        'num_leaves': 50,\n",
    "        'learning_rate': 0.06,\n",
    "        'min_data_in_leaf': 90,\n",
    "        'bagging_freq': 1,\n",
    "        'bagging_fraction': 0.8,\n",
    "        'lambda_l1': 0.3,\n",
    "        'lambda_l2': 0.1\n",
    "    },\n",
    "    '1JHN': {\n",
    "        'n_estimators': 100000,\n",
    "        'metric': 'mae',\n",
    "        'num_leaves': 10,\n",
    "        'learning_rate': 0.06,\n",
    "        'min_data_in_leaf': 80,\n",
    "        'bagging_freq': 1,\n",
    "        'bagging_fraction': 0.9,\n",
    "        'lambda_l1': 0.1,\n",
    "        'lambda_l2': 0.3\n",
    "    },\n",
    "    '2JHC': {\n",
    "        'n_estimators': 100000,\n",
    "        'metric': 'mae',\n",
    "        'num_leaves': 50,\n",
    "        'learning_rate': 0.1,\n",
    "        'min_data_in_leaf': 150,\n",
    "        'bagging_freq': 1,\n",
    "        'bagging_fraction': 0.9,\n",
    "        'lambda_l1': 0.3,\n",
    "        'lambda_l2': 0.1\n",
    "    },\n",
    "    '2JHH': {\n",
    "        'n_estimators': 100000,\n",
    "        'metric': 'mae',\n",
    "        'num_leaves': 30,\n",
    "        'learning_rate': 0.08,\n",
    "        'min_data_in_leaf': 120,\n",
    "        'bagging_freq': 1,\n",
    "        'bagging_fraction': 0.9,\n",
    "        'lambda_l1': 0.3,\n",
    "        'lambda_l2': 0.1\n",
    "    },\n",
    "    '2JHN': {\n",
    "        'n_estimators': 100000,\n",
    "        'metric': 'mae',\n",
    "        'num_leaves': 20,\n",
    "        'learning_rate': 0.06,\n",
    "        'min_data_in_leaf': 80,\n",
    "        'bagging_freq': 1,\n",
    "        'bagging_fraction': 0.8,\n",
    "        'lambda_l1': 0.3,\n",
    "        'lambda_l2': 0.1\n",
    "    },\n",
    "    '3JHC': {\n",
    "        'n_estimators': 100000,\n",
    "        'metric': 'mae',\n",
    "        'num_leaves': 80,\n",
    "        'learning_rate': 0.08,\n",
    "        'min_data_in_leaf': 150,\n",
    "        'bagging_freq': 1,\n",
    "        'bagging_fraction': 0.8,\n",
    "        'lambda_l1': 0.3,\n",
    "        'lambda_l2': 0.1\n",
    "    },\n",
    "    '3JHH': {\n",
    "        'n_estimators': 100000,\n",
    "        'metric': 'mae',\n",
    "        'num_leaves': 30,\n",
    "        'learning_rate': 0.08,\n",
    "        'min_data_in_leaf': 120,\n",
    "        'bagging_freq': 1,\n",
    "        'bagging_fraction': 0.8,\n",
    "        'lambda_l1': 0.1,\n",
    "        'lambda_l2': 0.3\n",
    "    },\n",
    "    '3JHN': {\n",
    "        'n_estimators': 100000,\n",
    "        'metric': 'mae',\n",
    "        'num_leaves': 30,\n",
    "        'learning_rate': 0.06,\n",
    "        'min_data_in_leaf': 80,\n",
    "        'bagging_freq': 1,\n",
    "        'bagging_fraction': 0.8,\n",
    "        'lambda_l1': 0.5,\n",
    "        'lambda_l2': 0.1\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for type_ in ['1JHC', '1JHN', '2JHC', '2JHH', '2JHN', '3JHC', '3JHH', '3JHN']:\n",
    "    print(f'Training type {type_}')\n",
    "    train = pd.read_csv(f'./feature_engineering/feature_output/each_type/train_{type_}.csv')\n",
    "    test = pd.read_csv(f'./feature_engineering/feature_output/each_type/test_{type_}.csv')\n",
    "    train_geo = pd.read_csv(f'./feature_engineering/feature_output/each_type_geo/train_geo_{type_}.csv')\n",
    "    test_geo = pd.read_csv(f'./feature_engineering/feature_output/each_type_geo/test_geo_{type_}.csv')\n",
    "    oof_submission = pd.DataFrame(train['id'])\n",
    "    submission = pd.DataFrame(test['id'])\n",
    "    \n",
    "    fc_train = pd.read_csv(f'./fc_predict/tuned/oof_prediction_{type_}.csv')\n",
    "    fc_test = pd.read_csv(f'./fc_predict/tuned/submission_{type_}.csv')\n",
    "    train['fc_predict'] = fc_train['oof']\n",
    "    test['fc_predict'] = fc_test['scalar_coupling_constant']\n",
    "\n",
    "\n",
    "    if type_[0] == '1':\n",
    "        train = train.drop(['Angle', 'cosA', 'cos2A', 'Torsion', 'cosT', 'cos2T'], axis=1)\n",
    "        test = test.drop(['Angle', 'cosA', 'cos2A', 'Torsion', 'cosT', 'cos2T'], axis=1)\n",
    "    if type_[0] == '2':\n",
    "        train = train.drop(['Torsion', 'cosT', 'cos2T'], axis=1)\n",
    "        test = test.drop(['Torsion', 'cosT', 'cos2T'], axis=1)\n",
    "    if type_[0] == '3':\n",
    "        train = train.drop(['Angle', 'cosA', 'cos2A'], axis=1)\n",
    "        test = test.drop(['Angle', 'cosA', 'cos2A'], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "    def drop_all_null_cols(df_train, df_test):\n",
    "        drop_cols = []\n",
    "        cols = df_train.columns\n",
    "\n",
    "        for atom_idx in ['atom_0', 'atom_1']:\n",
    "            for atom in ['H', 'C', 'N', 'O', 'F']:\n",
    "                for i in range(20):\n",
    "                    col = 'potential_' + atom + '_' + str(i) + '_' + atom_idx\n",
    "                    if col in cols:\n",
    "                        if df_train[col].isnull().all() or df_test[col].isnull().all():\n",
    "                            drop_cols.append(col)\n",
    "\n",
    "        df_train = df_train.drop(drop_cols, axis=1)\n",
    "        df_test = df_test.drop(drop_cols, axis=1)\n",
    "\n",
    "        return df_train, df_test\n",
    "\n",
    "    train, test = drop_all_null_cols(train, test)\n",
    "\n",
    "\n",
    "\n",
    "    train = train.fillna(0)\n",
    "    test = test.fillna(0)\n",
    "\n",
    "\n",
    "\n",
    "    train = pd.merge(train, train_geo, on=['id', 'atom_index_0', 'atom_index_1'], how='left')\n",
    "    test = pd.merge(test, test_geo, on=['id', 'atom_index_0', 'atom_index_1'], how='left')\n",
    "\n",
    "\n",
    "    drop_cols = [\n",
    "        'id', 'atom_index_0', 'atom_index_1', 'type',\n",
    "#         'vander_H_atom_0', 'vander_C_atom_0', 'vander_N_atom_0', 'vander_O_atom_0', 'vander_F_atom_0', 'Vander_atom_0', \n",
    "#         'vander_H_atom_1', 'vander_C_atom_1', 'vander_N_atom_1', 'vander_O_atom_1', 'vander_F_atom_1', 'Vander_atom_1'\n",
    "    ]\n",
    "    X = train.drop(drop_cols + ['scalar_coupling_constant', 'fc'], axis=1)\n",
    "    y = train['scalar_coupling_constant']\n",
    "    X_test = test.drop(drop_cols + ['molecule_name'], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "    del train, test\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "    result_dict = train_model(X, y, X_test, params[type_], is_plot=True)\n",
    "    \n",
    "    folder = './output/fc_tunedr'\n",
    "    if not os.path.exists(folder):\n",
    "        os.mkdir(folder)\n",
    "\n",
    "    submission['scalar_coupling_constant'] = result_dict['prediction']\n",
    "    submission.to_csv(f'{folder}/submission_{type_}.csv', index=False)\n",
    "    result_dict['feature_importance'].to_csv(f'./{folder}/feature_importance_{type_}.csv', index=False)\n",
    "    oof_submission['oof'] = result_dict['oof']\n",
    "    oof_submission.to_csv(f'{folder}/oof_prediction_{type_}.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
